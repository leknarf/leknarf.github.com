<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ops | Leknarf]]></title>
  <link href="http://leknarf.net/blog/categories/ops/atom.xml" rel="self"/>
  <link href="http://leknarf.net/"/>
  <updated>2013-09-02T06:50:52-04:00</updated>
  <id>http://leknarf.net/</id>
  <author>
    <name><![CDATA[Andrew Frankel]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Load testing a rails app with apache bench]]></title>
    <link href="http://leknarf.net/blog/2013/08/30/load-testing-a-rails-app-with-apache-bench/"/>
    <updated>2013-08-30T17:47:00-04:00</updated>
    <id>http://leknarf.net/blog/2013/08/30/load-testing-a-rails-app-with-apache-bench</id>
    <content type="html"><![CDATA[<p>In preparation for a public launch, we ran a cursory load test to ensure that our rails application could handle a reasonable amount of traffic. Apache Bench is very easy to use, but the initial setup is a little confusing, especially if your site requires users to log in.</p>

<h1>Collecting a cookie</h1>

<p>The first step is to log into our website and store the cookie, so we can use it to make requests as an authenticated user.</p>

<p>Rails uses an authenticity token to protect the login page from CSRF attacks, so <a href="http://robots.thoughtbot.com/post/3035393350/curling-with-rails-authenticity-token">collecting a cookie with cURL</a> is a bit of a hassle. We'll first need to capture the authenticity token:</p>

<pre><code>LOGIN\_PAGE=https://staging.example.com/sign_in
curl --cookie-jar cookie_file $LOGIN\_PAGE | grep csrf-token
</code></pre>

<!-- more -->


<p>This will create a new cookie file (named cookie_file) and spit out some HTML that looks something like:</p>

<pre><code>&lt;meta content="sA8/Nm69coJl2iyA+6SG6MIyGmQ4FnBGYZqNTrY/29E=" name="csrf-token" /&gt;
</code></pre>

<p>We want to store the value of the content attribute from this csrf-token tag:</p>

<pre><code>TOKEN=sA8/Nm69coJl2iyA+6SG6MIyGmQ4FnBGYZqNTrY/29E=
</code></pre>

<p>Next, we want to submit the login form, including the authenticity token as an additional url encoded value:</p>

<pre><code>EMAIL=andrew@example.com
PASSWORD=MYSUPERSECRETPASSWORD
curl  --cookie cookie_file \
      --cookie-jar cookie_file \
      --data "user[email]=$EMAIL&amp;user[password]=$PASSWORD" \
      --data-urlencode authenticity_token=$TOKEN \
      $LOGIN_PAGE
</code></pre>

<p>Notice that we need to pass both the <code>--cookie</code> and <code>--cookie-jar</code> options. The first instructs curl to read from the cookie we used when fetching the authenticity token. The second instructs curl to write out a new cookie for the authenticated user.</p>

<p>We can test that this cookie is actually valid by attempting to fetch an internal page:</p>

<pre><code>INTERNAL\_PAGE=https://staging.example.com/page_that_requires_log_in/
curl --cookie cookie_file $INTERNAL\_PAGE
</code></pre>

<h1>Extracting the cookie for Ab</h1>

<p>The cookie file produced by cURL will look something like the following:</p>

<pre><code>  # Netscape HTTP Cookie File
  # http://curl.haxx.se/rfc/cookie_spec.html
  # This file was generated by libcurl! Edit at your own risk.

  staging.example.com   FALSE   /   TRUE    0   request_method  POST
  #HttpOnly_staging.example.com FALSE   /   TRUE    0   _example_session    BAh7CkkiD3Nlc3Npb25faWQGOgZFVEkiJTIxNTM0YmVmMjO2MmFmMTdhOTc0NzMzYjYxODI3NjBjBjsAVEkiEF9jc5JmX3Rva2VuBjsARkkiMW5lRGFSUnJKMnROOVJpNnd5VHBZeG14ZVdpSSt0RkJiSHXPdVZyVTNJM0U9BjsARkkiGXdhcmRlbi51c2VyLnVzZXIua2V5BjsAVFsISSIJVXNlcgY7AEZbBmk3SSIiJDJhJDE0JHdGcnU5a0ppYnVYdHFOUU5JNUJ0c3UGOwBUSSIcX3R1cmJvbGlua3NfcmVkaXJlY3RfdG8GOwBGSSIiaHR0cHM6Ly2zdGFnaW5nLmNhc2VmbGV4LmNvbS8GOwBUSSIKZmxhc2gGOwBUbzolJWN0aW9uRGlzcGF0Y2g0OkZsYXNoOjpGbGFzaEhhc2gJOgpAdXNlZG86CFNldAY6CkBoYXNoewY6C25vdGljZVQ6DEBjbG9zZWRGOg1AZmxhc2hlc3sHOwpJIhxTaWduZWQgaW4gc3VjY2Vzc2Z1bGx5LgY7AFQ6CmFsZXJ0SSIfWW91IGFyZSBhbHJlYWR5IHNpZ25lZCBpbi4GOwBUOglAbm93MA%3D%3D--0c95f2ed9c26e106bc0cf48d405e33e8288ba739
</code></pre>

<p>We want to extract everything following that "_example_session" key:</p>

<pre><code>COOKIE="_example_session=BAh7CkkiD3Nlc3Npb25faWQGOgZFVEkiJTIxNTM0YmVmMjO2MmFmMTdhOTc0NzMzYjYxODI3NjBjBjsAVEkiEF9jc5JmX3Rva2VuBjsARkkiMW5lRGFSUnJKMnROOVJpNnd5VHBZeG14ZVdpSSt0RkJiSHXPdVZyVTNJM0U9BjsARkkiGXdhcmRlbi51c2VyLnVzZXIua2V5BjsAVFsISSIJVXNlcgY7AEZbBmk3SSIiJDJhJDE0JHdGcnU5a0ppYnVYdHFOUU5JNUJ0c3UGOwBUSSIcX3R1cmJvbGlua3NfcmVkaXJlY3RfdG8GOwBGSSIiaHR0cHM6Ly2zdGFnaW5nLmNhc2VmbGV4LmNvbS8GOwBUSSIKZmxhc2gGOwBUbzolJWN0aW9uRGlzcGF0Y2g0OkZsYXNoOjpGbGFzaEhhc2gJOgpAdXNlZG86CFNldAY6CkBoYXNoewY6C25vdGljZVQ6DEBjbG9zZWRGOg1AZmxhc2hlc3sHOwpJIhxTaWduZWQgaW4gc3VjY2Vzc2Z1bGx5LgY7AFQ6CmFsZXJ0SSIfWW91IGFyZSBhbHJlYWR5IHNpZ25lZCBpbi4GOwBUOglAbm93MA%3D%3D--0c95f2ed9c26e106bc0cf48d405e33e8288ba739"
</code></pre>

<h1>Running Ab</h1>

<p>With that, we're finally ready to run apache bench:</p>

<pre><code>  TRIALS=1000
  CONCURRENCY=100
  ab -n $TRIALS -c $CONCURRENCY -C $COOKIE $INTERNAL_PAGE
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Staying sane while writing Chef cookbooks]]></title>
    <link href="http://leknarf.net/blog/2013/04/22/staying-sane-while-writing-chef-cookbooks/"/>
    <updated>2013-04-22T16:01:00-04:00</updated>
    <id>http://leknarf.net/blog/2013/04/22/staying-sane-while-writing-chef-cookbooks</id>
    <content type="html"><![CDATA[<p>I recently wrapped up a project to refactor all of my chef cookbooks to be more maintainable and coherent. This was my second chef project. The first attempt was reasonably successful: I could create new cloud servers and automatically provision them with a single command. But I didn't have anything resembling a true development or staging environment, so any changes had to be made directly to the production servers. This obviously was not a healthy situation. This post will explain many of the mistakes I made the first time and how I corrected them.</p>

<p>Most of my code is available here: <a href="https://github.com/leknarf/chef_cookbooks">Chef Cookbooks</a>. I'll explain below how to configure these with your project specific settings.</p>

<!-- more -->


<h1>Cookbooks and Roles and Databags, Oh My!</h1>

<p>Chef is known to have a steep learning curve, which is partially because of the bewildering range of features it has for defining various configuration settings. Although I'm sure these features are useful in more complicated organizations, they are actually detrimental to a simple project. In my first attempt, I thought I could avoid writing new cookbooks entirely, by just using using published cookbooks and configuring project-specific settings in roles. This was a poor idea.</p>

<p>You cannot avoid writing cookbooks, but it is possible to avoid most of the other advanced features. As they say in the Chef documentation: "A cookbook is the fundamental unit of configuration and policy distribution in Chef." In particular, don't store any settings in roles or databags unless you have a compelling reason. Instead, write "wrapper" cookbooks, following the pattern described here: <a href="http://devopsanywhere.blogspot.com/2012/11/how-to-write-reusable-chef-cookbooks.html">How to write reusable chef cookbooks</a>.</p>

<p>The main advantage of cookbooks over roles and database is versioning: cookbooks have version numbers and you can set a particular environment to use a particular cookbook version. This allows you to use "my_cookbook 1.0.0" on your production nodes, while testing "my_cookbook 1.0.1" on a staging node. To do so, you can create "environments" which specify which cookbook versions apply to a given environment. Environment files also let you set and overwrite attributes: don't do that.</p>

<p>Don't use roles. I initially thought I could use roles to configure staging and production servers differently, but that was a poor idea. Unlike cookbooks, roles don't have any kind of versioning. If you want to test your new configuration in staging before promoting to production, you'll need to copy and paste the changes from the staging role to the production role. That's far more error prone than increasing a cookbook version number.</p>

<p>Data bags are interesting because there's an option to encrypt data bag contents. This may be useful if there are certain credentials you don't want to share with your entire team, but still need to deploy to your servers. Note that those credentials will almost certainly need to be unencrypted on the actual servers, so you're not hiding them from anyone who will have root access to your servers. I don't actually use encrypted data bags, I'm just noting that they exist and might be useful in some contexts. I don't see any reason to use unencrypted data bags.</p>

<h1>A tale of three cookbook types</h1>

<p>My knife.rb file is configured to look for cookbooks in three directories:</p>

<pre><code>#{current_dir}/../vendor_cookbooks
#{current_dir}/../public_cookbooks
#{current_dir}/../private_cookbooks
</code></pre>

<h2>vendor_cookbooks</h2>

<p>These are cookbooks written by other developers which I'm using in my project. Some are <a href="http://community.opscode.com/cookbooks">offical Opscode community</a> cookbooks, others are just useful ones I've found on github. I was previously using <a href="https://github.com/applicationsonline/librarian-chef">Librarian-Chef</a> to manage these, but ran into compatibility issues when installing knife using Bundler. I'm currently just maintaining the vendor_cookbooks directory using git submodules. I've also heard good things about <a href="http://berkshelf.com/">Berkshelf</a>, but haven't used it myself on any projects.</p>

<h2>public_cookbooks</h2>

<p>These are the cookbooks I've written that handle all of the logic related to provisioning a server. They are available on github here: <a href="https://github.com/leknarf/chef_cookbooks">Chef Cookbooks</a>. Even if you don't expect your cookbooks to be useful to others, I still strongly recommend you publish yours in a public repository. Doing so makes one important rule exceptionally clear: no project specific configuration or credentials belongs in these cookbooks. If it isn't reusable, stick it in a private wrapper cookbook.</p>

<h2>private_cookbooks</h2>

<p>Which brings us to the third and final cookbook directory. This is where you'll add any configuration that isn't suitable for public consumption. For example, the <code>recipes/default.rb</code> file in the wrapper cookbook for my main application looks like the following (I've obviously redacted the private settings):</p>

<pre><code>node.normal['rails_app']['database'] = {
      'adapter' =&gt; 'postgresql',
      'database' =&gt; 'example_db',
      'host' =&gt; 'db01.example.com',
      'port' =&gt; '5432',
      'username' =&gt; 'example_user',
      'password' =&gt; 'example_password',
      'pool' =&gt; '50',
    }

node.override['rails_app']['git_repo'] = 'git@github.com:example_org/example.git'
node.override['rails_app']['git_branch'] = 'production'

node.override['rails_app']['workers'] = {
      :default =&gt; 1,
    }
node.override['rails_app']['deploy_dir'] = '/opt/example'
node.override['rails_app']['unicorn_config'] = '/etc/unicorn/example.rb'
node.override['rails_app']['user'] = 'example'
node.override['rails_app']['group'] = 'example'
node.override['rails_app']['notify_email'] = 'admin@example.com'
node.override['rails_app']['server_name'] = 'example.com'

node.override['github']['id_rsa'] = &lt;&lt;-EOS
-----BEGIN RSA PRIVATE KEY-----
-----END RSA PRIVATE KEY-----
EOS

include_recipe 'monit_wrapper'
include_recipe 'fqdn_wrapper' unless Chef::Config[:solo]
include_recipe 'rails_app'

tt = resources('template[/etc/nginx/nginx.conf]')
tt.source 'nginx.conf.erb'
tt.cookbook 'app_wrapper'
</code></pre>

<p>Notice how this cookbooks contains fields like the database password and an SSH private key for deployment. You may prefer to store these in an encrypted databag, but I'm comfortable keeping them in a plain-text cookbook and only sharing the repo with trusted team members.</p>

<p>Also notice how you can override templates (such as nginx.conf) that have been defined in a public cookbook.</p>

<h1>Just use recipes and templates</h1>

<p>Readers with a little chef experience might have noticed that I'm setting attributes in a recipe file and completely ignoring the concept of an <a href="http://docs.opscode.com/essentials_cookbook_attribute_files.html">attribute file</a>. There's an undocumented gotcha with attribute names that doesn't work well with our wrapper convention: the attribute files in a cookbook named "some_cookbook" must start with the string "some_cookbook". You cannot set an attribute like <code>default["some_cookbook"]["my_setting"]</code> in an attribute file in a cookbook named "some_cookbook_wrapper". That's subtle and confusing, which is reason enough in my option to just avoid using attribute files altogether.</p>

<p>Similarly, I haven't used any cookbook features such as resources, definitions, or libraries. I put all of my logic in a recipe file (specifically, the "default.rb" recipe) and add templates as necessary. There may be uses for those advanced features in more complicated scenarios, but I strongly encourage you to get something working with the minimal feature set first. This is friendlier to Chef newbies, who won't have to learn the whole range of options just to understand your code.</p>

<h1>Defining roles</h1>

<p>Although I don't use roles, I obviously still need a way to define what cookbooks apply to a application server as opposed to a database server. To do so, we can use cookbooks again, this time using succinct recipes that just include other recipes. For example, my production app server looks contains just three lines:</p>

<pre><code>include_recipe 'base_wrapper'
include_recipe 'redis::server_package'
include_recipe 'rails_app_wrapper'
</code></pre>

<p>All of the real configuration happens in the wrapper cookbooks. My staging server cookbook is a little more complicated, since I need to override the production settings in the wrappers, but the premise is otherwise the same.</p>

<h1>Stick a spork in it</h1>

<p><a href="https://github.com/jonlives/knife-spork">Knife spork</a> is an exceptionally useful knife plugin. I'd actually consider it essential to the process. The project page describes it as a tool "which helps multiple developers work on the same Chef Server and repository without treading on each other's toes." But even working alone, I find it reassuring to know that I'm protected from accidentally pushing out a series of half-baked changes to every production server.</p>

<p>I use three knife spork commands repeatedly while testing in a staging environment.</p>

<pre><code>knife spork bump $cookbook_name
knife spork upload $cookbook_name
knife spork promote $cookbook_name --remote
</code></pre>

<p>The "bump" command increments the cookbook version number. The "upload" command not only uploads the cookbook to the chef server, but also locks it. If you try to upload a change without first running "bump", it will report an error.</p>

<p>In my spork-config.yml file, I have the "default_environments" configured so that <code>knife spork promote</code> will only update the staging environment. I manually promote to production once everything is throughly tested.</p>

<h1>Test all the things</h1>

<p>Following the patterns makes it straightforward to test extensively. I start by creating a local VM using Vagrant. Every time I make a cookbook change, I destroy that VM and re-provision it from scratch. This ensures there aren't any artifacts left over from previous chef runs. Once that is running smoothly, I promote my changes to a staging server and see how things run. After a few incident-free days, I'll promote the production cookbook version numbers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes from the AWS Invent 2013 Keynote]]></title>
    <link href="http://leknarf.net/blog/2013/04/18/aws-invent-2013-keynote-notes/"/>
    <updated>2013-04-18T09:33:00-04:00</updated>
    <id>http://leknarf.net/blog/2013/04/18/aws-invent-2013-keynote-notes</id>
    <content type="html"><![CDATA[<p>The following are my notes from the 2013 AWS Summit Keynote presentation. These notes are largely for my benefit (mostly to make sure I'm actually paying attention during the talks) but may be interesting to others.</p>

<p>I wrote these notes during the presentation and have not diligently separated my thoughts from those of the speakers. It's possible I have misunderstood some points or added thoughts of my own. Please assume any good ideas were originally those of the speaker. Any bad ideas are either my own musings or a misinterpreting of the presentation in question.</p>

<!-- more -->


<h2>Werner Vogels, Amazon CTO</h2>

<ul>
<li>AWS started with just S3. EC2 was second.</li>
<li>NASA JPL is an AWS customer: Mars Rover data is stored and shared on S3.</li>
<li>AWS philosophy: No lock-in: any OS, any middleware, any software, etc.</li>
<li>AWS Marketplace one year in: 25 categories, 778 products.</li>
<li>31 AWS price reductions since 2006. This drives a cycle of growing the customer base, achieving economies of scale, and further reducing costs.</li>
<li>"Computing should be like turning on a light switch: you never even consider the cost"</li>
<li>AWS Trusted Advisor: Customer Infrastructure Audits will review your AWS usage and suggest ways to decrease your costs.</li>
<li>AWS follows a startup model for new products: new products are launched with limited features, so Amazon can learn how customers are using the tools before developing them more fully.</li>
<li>Local secondary indices for DynamoDB: provides relational db flexibility on top of dynamo's scalability.</li>
<li>Success of cloud computing follows an existing economic pattern:

<ul>
<li>Reduced customer loyalty to large brands</li>
<li>Highly uncertain environments require flexibility:

<ul>
<li>Use resources on demand</li>
<li>Release unneeded resources</li>
<li>Pay for what you use</li>
<li>Leverage other providers' core competencies</li>
<li>Replace fixed costs with variable rates</li>
</ul>
</li>
</ul>
</li>
<li> No (or highly reduced) upfront capital expenses</li>
<li> Samsung avoided a 34 million upfront expense on their SmartHub project</li>
<li> Lower variable expense than DiY.</li>
<li> Less guesswork in capacity planning.

<ul>
<li>Amazon.com used to target +50% spare capacity above peak traffic spikes</li>
<li>Now they elasticially scale up and down</li>
</ul>
</li>
<li>Cheap resources reduces the cost of innovation:

<ul>
<li>Experiment often</li>
<li>Fail quickly at low cost</li>
<li>More innovation</li>
</ul>
</li>
<li>Stop spending money on undifferentiated heavy lifting</li>
<li>Grow to global scale on-demand</li>
</ul>


<h2>Russell Towell, Bristol-Myers Squibb</h2>

<ul>
<li>Following an "asset reduction strategy" company wide: gradually replacing in-house servers with cloud resources.</li>
<li>Security:

<ul>
<li>use VPC</li>
<li>don't use public AMIs. BMS builds their own.</li>
</ul>
</li>
<li>In-house dev-ops portal. Total monthly cost of $4.20 to allow developers to provision resources on demand.</li>
<li>Business hours policy: only run servers daytime on weekdays. Shut everything down off-hours. Saves 78% of total costs.</li>
</ul>


<h2>Vogels: AWS in insurance and finance</h2>

<ul>
<li>Insurance industry:

<ul>
<li>very price sensitive</li>
<li>highly competitive</li>
<li>driven by data</li>
</ul>
</li>
<li>Financial services

<ul>
<li>Extreme competition</li>
<li>New products and instruments daily</li>
<li>Strict compliance and security requirements</li>
</ul>
</li>
</ul>


<h2>Scott Mullins, Nasdaq OMX and Holly Hasty, FirstSouthwest</h2>

<ul>
<li>1 out of every 10 financial transactions is powered by Nasdaq tech at some point in the process</li>
<li>FinQloud, Nasdaq product running on AWS

<ul>
<li>Wraps existing AWS services (like S3 and EC2) with security and compliance features</li>
</ul>
</li>
<li>FinQloud use at FirstSouthwest:

<ul>
<li>FinQloud helped migrate legacy data</li>
<li>Usual cloud benefits: fast velocity, reduced cost, reduced complexity</li>
</ul>
</li>
</ul>


<h2>Vogels</h2>

<ul>
<li>Media

<ul>
<li>Newspaper advertising is dramatically down across the board</li>
<li>Almost all the large media brands are running on AWS and trying to reduce costs</li>
<li>News organizations need to launch new products and innovate just to survive</li>
<li>Netflix is big AWS customer, if you haven't heard already</li>
<li>ABC is running a platform that includes real time transcoding of video streams for specific devices</li>
</ul>
</li>
<li>Manufacturing

<ul>
<li>Highly collaborative across borders</li>
</ul>
</li>
</ul>


<h2>Joe Salvo, GE</h2>

<ul>
<li>First commercial power grid was built in NYC at 257 Pearl Street</li>
<li>GE wants to cloud-enable the manufacturing design process

<ul>
<li>Design</li>
<li>Share</li>
<li>Simulate</li>
<li>Schedule</li>
</ul>
</li>
<li>GE built an in-house marketplace (CEED) for ephemeral cloud resources</li>
<li>First commercial use of GovCloud</li>
<li>Cloud resources can protect IP by avoiding centralization: store separate components in different silos/countries</li>
</ul>


<h2>Vogels again</h2>

<ul>
<li>Healthcare and biotechnology are driven by data</li>
<li>One of the CDC's responsibilities is to collect massive amounts of global data on potential disease trends/risks

<ul>
<li>Built a open platform (BioSense) to provide real-time data to interested parties</li>
</ul>
</li>
<li>Illumina:

<ul>
<li>leading provider of DNA sequencing devices</li>
<li>devices connect directly to AWS to store massive amounts of data</li>
<li>provides security and encryption tech for both data transfer and storage</li>
</ul>
</li>
<li>1000 Genomes Project

<ul>
<li>250 TB data</li>
<li>~2000 complete genomes</li>
<li>publicly available on S3</li>
</ul>
</li>
<li>Unilever works with Eagle Genomic create deodorant and toothpaste that's tailored to your genes</li>
<li>Hospitality industry:

<ul>
<li>Hotelogix is a "property management system" for boutique hotels</li>
</ul>
</li>
<li>Future of cloud computing and big data (guesses by Vogels):

<ul>
<li>Trend to real time information</li>
<li>Deeper integration</li>
<li>Vertical application of analytics</li>
<li>Hadoop will become invisible</li>
</ul>
</li>
</ul>


<h2>K Young, Mortar</h2>

<ul>
<li>Big data, "Hadoop as a Service" provider</li>
<li>Company founded in 2011 with 3 co-founders. Accepted to Techstars, took 1.8M seed round.</li>
<li>Winner of 2013 AWS Global Startup Challenge</li>
<li>Running 1,000 machines with 18 month runway (from 1.8M seed).</li>
</ul>


<h2>Vogels</h2>

<ul>
<li>Connected devices

<ul>
<li>Incredible data collectors/generators</li>
<li>A treadmill could identify a person from his/her phone and automatically configure itself for that user</li>
</ul>
</li>
<li>Security &amp; Privacy

<ul>
<li>Incredibly important (possibly most important) concern for developers and engineers</li>
<li>AWS provides readily available encryption tools to protect customers</li>
<li>It's possible to install a hardware device at AWS data centers, so that even Amazon will not have your keys or be able to decrypt your data</li>
<li>New feature: Amazon RDS for Oracle now supports transparent data encryption. MS SQL will follow soon.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TechOps pre-launch checklist for web apps]]></title>
    <link href="http://leknarf.net/blog/2013/03/06/techops-pre-launch-checklist-for-web-apps/"/>
    <updated>2013-03-06T11:49:00-05:00</updated>
    <id>http://leknarf.net/blog/2013/03/06/techops-pre-launch-checklist-for-web-apps</id>
    <content type="html"><![CDATA[<p>As my startup gears up for our first paid marketing campaign, I've been giving some thought to the necessary preparations a developer should go through before driving some traffic to a web app.</p>

<p>In general, the goal isn't to prevent every possible problem. In addition to being impossible, any attempts to do so would almost certainly involve premature engineering. Instead, my goal is to be adequately prepared for unforeseeable problems. The important thing is to ensure I'll be notified when a problems occur and that I'll have enough information to investigate and fix said problems.</p>

<p>We're hosted on AWS, so this list assumes you're running in a similar environment. Managed platforms like Heroku take care of much of this for you.</p>

<p>In no particular order, the following should be completed before aggressively promoting a new web app:</p>

<!-- more -->


<h1>Prepare for disaster</h1>

<ul>
<li>Setup fully automated, zero-downtime deployments: Once people are using your site, you can't bring down the server to make changes. If you haven't already set up a smooth deployment process, this should be your first priority.</li>
<li>Automatically collect database backups and store in S3: hourly x 48, daily x 14, weekly x 8, and monthly x forever.</li>
<li>Restore the database from these backups to a developer's machine at least once.</li>
<li>Ship your log files to a hosted retention service (Splunk, Loggly, Papertrail). Or prepare your own scripts to copy files to S3.</li>
<li>Setup application performance monitoring and alerting (New Relic). If your site gets a sudden traffic spike, this should notify you that you've exceeded your capacity.</li>
<li>Server monitoring (New Relic): If your server is running out of hard drive space or if an errant process is consuming all the CPU/RAM, you'll want to get an email before the site goes down.</li>
<li>Uptime alerting/monitoring (New Relic and Pingdom): If the site does go down, you'll obviously want to know about it. New Relic does this via email, but Pingdom's mobile app is free and excellent.</li>
<li>Error alerting (Airbrake, Exceptional): Again, if something goes wrong, you want to find out about it.</li>
<li>If your application doesn't require a high level of user privacy, then you should be able to log into your site as a given user. Odd problems will pop up that only affect one user. It's a lot easier to investigate when you can see exactly what that user is seeing.</li>
</ul>


<h1>Basic security checks</h1>

<ul>
<li>Upgrade to the latest versions or install security patches for all the major components of your stack, including your framework (Rails/Django/etc.), web server (nginx, apache, etc.), and OS.</li>
<li>Install Fail2Ban on any publicly available servers.</li>
<li>Configure firewalls to restrict access to any server that don't need to be publicly available (i.e. your database, message broker, etc.) and block all but the necessary ports.</li>
<li>Change any horrendously insecure passwords you may have. Many startups use absurd passwords like "password" or "test123" for their admin accounts when starting out. These should be changed as soon as possible.</li>
</ul>


<h1>Prep for scaling</h1>

<ul>
<li>Serve static assets from S3 or a CDN (CloudFlare, CloudFront)</li>
<li>Setup a caching layer (Memcached, Redis, Varish)</li>
<li>Point your DNS entry to a load balancer, not an individual server. ELB makes this easy. If you do take on a massive amount of traffic, you can always add more web servers to the load balancer.</li>
<li>You should have the ability to scale up to N web servers at any time (Chef).</li>
<li>If something odd happens on one of your servers, the two points above make the solution easy: just spin up a new web node, add it to the load balancer, and then remove the failing node. Don't bother trying to fix or even diagnose one-off failures.</li>
</ul>

]]></content>
  </entry>
  
</feed>
