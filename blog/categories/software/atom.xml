<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: software | Leknarf]]></title>
  <link href="http://leknarf.net/blog/categories/software/atom.xml" rel="self"/>
  <link href="http://leknarf.net/"/>
  <updated>2013-09-02T06:50:52-04:00</updated>
  <id>http://leknarf.net/</id>
  <author>
    <name><![CDATA[Andrew Frankel]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Load testing a rails app with apache bench]]></title>
    <link href="http://leknarf.net/blog/2013/08/30/load-testing-a-rails-app-with-apache-bench/"/>
    <updated>2013-08-30T17:47:00-04:00</updated>
    <id>http://leknarf.net/blog/2013/08/30/load-testing-a-rails-app-with-apache-bench</id>
    <content type="html"><![CDATA[<p>In preparation for a public launch, we ran a cursory load test to ensure that our rails application could handle a reasonable amount of traffic. Apache Bench is very easy to use, but the initial setup is a little confusing, especially if your site requires users to log in.</p>

<h1>Collecting a cookie</h1>

<p>The first step is to log into our website and store the cookie, so we can use it to make requests as an authenticated user.</p>

<p>Rails uses an authenticity token to protect the login page from CSRF attacks, so <a href="http://robots.thoughtbot.com/post/3035393350/curling-with-rails-authenticity-token">collecting a cookie with cURL</a> is a bit of a hassle. We'll first need to capture the authenticity token:</p>

<pre><code>LOGIN\_PAGE=https://staging.example.com/sign_in
curl --cookie-jar cookie_file $LOGIN\_PAGE | grep csrf-token
</code></pre>

<!-- more -->


<p>This will create a new cookie file (named cookie_file) and spit out some HTML that looks something like:</p>

<pre><code>&lt;meta content="sA8/Nm69coJl2iyA+6SG6MIyGmQ4FnBGYZqNTrY/29E=" name="csrf-token" /&gt;
</code></pre>

<p>We want to store the value of the content attribute from this csrf-token tag:</p>

<pre><code>TOKEN=sA8/Nm69coJl2iyA+6SG6MIyGmQ4FnBGYZqNTrY/29E=
</code></pre>

<p>Next, we want to submit the login form, including the authenticity token as an additional url encoded value:</p>

<pre><code>EMAIL=andrew@example.com
PASSWORD=MYSUPERSECRETPASSWORD
curl  --cookie cookie_file \
      --cookie-jar cookie_file \
      --data "user[email]=$EMAIL&amp;user[password]=$PASSWORD" \
      --data-urlencode authenticity_token=$TOKEN \
      $LOGIN_PAGE
</code></pre>

<p>Notice that we need to pass both the <code>--cookie</code> and <code>--cookie-jar</code> options. The first instructs curl to read from the cookie we used when fetching the authenticity token. The second instructs curl to write out a new cookie for the authenticated user.</p>

<p>We can test that this cookie is actually valid by attempting to fetch an internal page:</p>

<pre><code>INTERNAL\_PAGE=https://staging.example.com/page_that_requires_log_in/
curl --cookie cookie_file $INTERNAL\_PAGE
</code></pre>

<h1>Extracting the cookie for Ab</h1>

<p>The cookie file produced by cURL will look something like the following:</p>

<pre><code>  # Netscape HTTP Cookie File
  # http://curl.haxx.se/rfc/cookie_spec.html
  # This file was generated by libcurl! Edit at your own risk.

  staging.example.com   FALSE   /   TRUE    0   request_method  POST
  #HttpOnly_staging.example.com FALSE   /   TRUE    0   _example_session    BAh7CkkiD3Nlc3Npb25faWQGOgZFVEkiJTIxNTM0YmVmMjO2MmFmMTdhOTc0NzMzYjYxODI3NjBjBjsAVEkiEF9jc5JmX3Rva2VuBjsARkkiMW5lRGFSUnJKMnROOVJpNnd5VHBZeG14ZVdpSSt0RkJiSHXPdVZyVTNJM0U9BjsARkkiGXdhcmRlbi51c2VyLnVzZXIua2V5BjsAVFsISSIJVXNlcgY7AEZbBmk3SSIiJDJhJDE0JHdGcnU5a0ppYnVYdHFOUU5JNUJ0c3UGOwBUSSIcX3R1cmJvbGlua3NfcmVkaXJlY3RfdG8GOwBGSSIiaHR0cHM6Ly2zdGFnaW5nLmNhc2VmbGV4LmNvbS8GOwBUSSIKZmxhc2gGOwBUbzolJWN0aW9uRGlzcGF0Y2g0OkZsYXNoOjpGbGFzaEhhc2gJOgpAdXNlZG86CFNldAY6CkBoYXNoewY6C25vdGljZVQ6DEBjbG9zZWRGOg1AZmxhc2hlc3sHOwpJIhxTaWduZWQgaW4gc3VjY2Vzc2Z1bGx5LgY7AFQ6CmFsZXJ0SSIfWW91IGFyZSBhbHJlYWR5IHNpZ25lZCBpbi4GOwBUOglAbm93MA%3D%3D--0c95f2ed9c26e106bc0cf48d405e33e8288ba739
</code></pre>

<p>We want to extract everything following that "_example_session" key:</p>

<pre><code>COOKIE="_example_session=BAh7CkkiD3Nlc3Npb25faWQGOgZFVEkiJTIxNTM0YmVmMjO2MmFmMTdhOTc0NzMzYjYxODI3NjBjBjsAVEkiEF9jc5JmX3Rva2VuBjsARkkiMW5lRGFSUnJKMnROOVJpNnd5VHBZeG14ZVdpSSt0RkJiSHXPdVZyVTNJM0U9BjsARkkiGXdhcmRlbi51c2VyLnVzZXIua2V5BjsAVFsISSIJVXNlcgY7AEZbBmk3SSIiJDJhJDE0JHdGcnU5a0ppYnVYdHFOUU5JNUJ0c3UGOwBUSSIcX3R1cmJvbGlua3NfcmVkaXJlY3RfdG8GOwBGSSIiaHR0cHM6Ly2zdGFnaW5nLmNhc2VmbGV4LmNvbS8GOwBUSSIKZmxhc2gGOwBUbzolJWN0aW9uRGlzcGF0Y2g0OkZsYXNoOjpGbGFzaEhhc2gJOgpAdXNlZG86CFNldAY6CkBoYXNoewY6C25vdGljZVQ6DEBjbG9zZWRGOg1AZmxhc2hlc3sHOwpJIhxTaWduZWQgaW4gc3VjY2Vzc2Z1bGx5LgY7AFQ6CmFsZXJ0SSIfWW91IGFyZSBhbHJlYWR5IHNpZ25lZCBpbi4GOwBUOglAbm93MA%3D%3D--0c95f2ed9c26e106bc0cf48d405e33e8288ba739"
</code></pre>

<h1>Running Ab</h1>

<p>With that, we're finally ready to run apache bench:</p>

<pre><code>  TRIALS=1000
  CONCURRENCY=100
  ab -n $TRIALS -c $CONCURRENCY -C $COOKIE $INTERNAL_PAGE
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes from the AWS Invent 2013 Keynote]]></title>
    <link href="http://leknarf.net/blog/2013/04/18/aws-invent-2013-keynote-notes/"/>
    <updated>2013-04-18T09:33:00-04:00</updated>
    <id>http://leknarf.net/blog/2013/04/18/aws-invent-2013-keynote-notes</id>
    <content type="html"><![CDATA[<p>The following are my notes from the 2013 AWS Summit Keynote presentation. These notes are largely for my benefit (mostly to make sure I'm actually paying attention during the talks) but may be interesting to others.</p>

<p>I wrote these notes during the presentation and have not diligently separated my thoughts from those of the speakers. It's possible I have misunderstood some points or added thoughts of my own. Please assume any good ideas were originally those of the speaker. Any bad ideas are either my own musings or a misinterpreting of the presentation in question.</p>

<!-- more -->


<h2>Werner Vogels, Amazon CTO</h2>

<ul>
<li>AWS started with just S3. EC2 was second.</li>
<li>NASA JPL is an AWS customer: Mars Rover data is stored and shared on S3.</li>
<li>AWS philosophy: No lock-in: any OS, any middleware, any software, etc.</li>
<li>AWS Marketplace one year in: 25 categories, 778 products.</li>
<li>31 AWS price reductions since 2006. This drives a cycle of growing the customer base, achieving economies of scale, and further reducing costs.</li>
<li>"Computing should be like turning on a light switch: you never even consider the cost"</li>
<li>AWS Trusted Advisor: Customer Infrastructure Audits will review your AWS usage and suggest ways to decrease your costs.</li>
<li>AWS follows a startup model for new products: new products are launched with limited features, so Amazon can learn how customers are using the tools before developing them more fully.</li>
<li>Local secondary indices for DynamoDB: provides relational db flexibility on top of dynamo's scalability.</li>
<li>Success of cloud computing follows an existing economic pattern:

<ul>
<li>Reduced customer loyalty to large brands</li>
<li>Highly uncertain environments require flexibility:

<ul>
<li>Use resources on demand</li>
<li>Release unneeded resources</li>
<li>Pay for what you use</li>
<li>Leverage other providers' core competencies</li>
<li>Replace fixed costs with variable rates</li>
</ul>
</li>
</ul>
</li>
<li> No (or highly reduced) upfront capital expenses</li>
<li> Samsung avoided a 34 million upfront expense on their SmartHub project</li>
<li> Lower variable expense than DiY.</li>
<li> Less guesswork in capacity planning.

<ul>
<li>Amazon.com used to target +50% spare capacity above peak traffic spikes</li>
<li>Now they elasticially scale up and down</li>
</ul>
</li>
<li>Cheap resources reduces the cost of innovation:

<ul>
<li>Experiment often</li>
<li>Fail quickly at low cost</li>
<li>More innovation</li>
</ul>
</li>
<li>Stop spending money on undifferentiated heavy lifting</li>
<li>Grow to global scale on-demand</li>
</ul>


<h2>Russell Towell, Bristol-Myers Squibb</h2>

<ul>
<li>Following an "asset reduction strategy" company wide: gradually replacing in-house servers with cloud resources.</li>
<li>Security:

<ul>
<li>use VPC</li>
<li>don't use public AMIs. BMS builds their own.</li>
</ul>
</li>
<li>In-house dev-ops portal. Total monthly cost of $4.20 to allow developers to provision resources on demand.</li>
<li>Business hours policy: only run servers daytime on weekdays. Shut everything down off-hours. Saves 78% of total costs.</li>
</ul>


<h2>Vogels: AWS in insurance and finance</h2>

<ul>
<li>Insurance industry:

<ul>
<li>very price sensitive</li>
<li>highly competitive</li>
<li>driven by data</li>
</ul>
</li>
<li>Financial services

<ul>
<li>Extreme competition</li>
<li>New products and instruments daily</li>
<li>Strict compliance and security requirements</li>
</ul>
</li>
</ul>


<h2>Scott Mullins, Nasdaq OMX and Holly Hasty, FirstSouthwest</h2>

<ul>
<li>1 out of every 10 financial transactions is powered by Nasdaq tech at some point in the process</li>
<li>FinQloud, Nasdaq product running on AWS

<ul>
<li>Wraps existing AWS services (like S3 and EC2) with security and compliance features</li>
</ul>
</li>
<li>FinQloud use at FirstSouthwest:

<ul>
<li>FinQloud helped migrate legacy data</li>
<li>Usual cloud benefits: fast velocity, reduced cost, reduced complexity</li>
</ul>
</li>
</ul>


<h2>Vogels</h2>

<ul>
<li>Media

<ul>
<li>Newspaper advertising is dramatically down across the board</li>
<li>Almost all the large media brands are running on AWS and trying to reduce costs</li>
<li>News organizations need to launch new products and innovate just to survive</li>
<li>Netflix is big AWS customer, if you haven't heard already</li>
<li>ABC is running a platform that includes real time transcoding of video streams for specific devices</li>
</ul>
</li>
<li>Manufacturing

<ul>
<li>Highly collaborative across borders</li>
</ul>
</li>
</ul>


<h2>Joe Salvo, GE</h2>

<ul>
<li>First commercial power grid was built in NYC at 257 Pearl Street</li>
<li>GE wants to cloud-enable the manufacturing design process

<ul>
<li>Design</li>
<li>Share</li>
<li>Simulate</li>
<li>Schedule</li>
</ul>
</li>
<li>GE built an in-house marketplace (CEED) for ephemeral cloud resources</li>
<li>First commercial use of GovCloud</li>
<li>Cloud resources can protect IP by avoiding centralization: store separate components in different silos/countries</li>
</ul>


<h2>Vogels again</h2>

<ul>
<li>Healthcare and biotechnology are driven by data</li>
<li>One of the CDC's responsibilities is to collect massive amounts of global data on potential disease trends/risks

<ul>
<li>Built a open platform (BioSense) to provide real-time data to interested parties</li>
</ul>
</li>
<li>Illumina:

<ul>
<li>leading provider of DNA sequencing devices</li>
<li>devices connect directly to AWS to store massive amounts of data</li>
<li>provides security and encryption tech for both data transfer and storage</li>
</ul>
</li>
<li>1000 Genomes Project

<ul>
<li>250 TB data</li>
<li>~2000 complete genomes</li>
<li>publicly available on S3</li>
</ul>
</li>
<li>Unilever works with Eagle Genomic create deodorant and toothpaste that's tailored to your genes</li>
<li>Hospitality industry:

<ul>
<li>Hotelogix is a "property management system" for boutique hotels</li>
</ul>
</li>
<li>Future of cloud computing and big data (guesses by Vogels):

<ul>
<li>Trend to real time information</li>
<li>Deeper integration</li>
<li>Vertical application of analytics</li>
<li>Hadoop will become invisible</li>
</ul>
</li>
</ul>


<h2>K Young, Mortar</h2>

<ul>
<li>Big data, "Hadoop as a Service" provider</li>
<li>Company founded in 2011 with 3 co-founders. Accepted to Techstars, took 1.8M seed round.</li>
<li>Winner of 2013 AWS Global Startup Challenge</li>
<li>Running 1,000 machines with 18 month runway (from 1.8M seed).</li>
</ul>


<h2>Vogels</h2>

<ul>
<li>Connected devices

<ul>
<li>Incredible data collectors/generators</li>
<li>A treadmill could identify a person from his/her phone and automatically configure itself for that user</li>
</ul>
</li>
<li>Security &amp; Privacy

<ul>
<li>Incredibly important (possibly most important) concern for developers and engineers</li>
<li>AWS provides readily available encryption tools to protect customers</li>
<li>It's possible to install a hardware device at AWS data centers, so that even Amazon will not have your keys or be able to decrypt your data</li>
<li>New feature: Amazon RDS for Oracle now supports transparent data encryption. MS SQL will follow soon.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Find out when to post on hacker news]]></title>
    <link href="http://leknarf.net/blog/2013/03/13/find-out-when-to-post-on-hacker-news/"/>
    <updated>2013-03-13T16:15:00-04:00</updated>
    <id>http://leknarf.net/blog/2013/03/13/find-out-when-to-post-on-hacker-news</id>
    <content type="html"><![CDATA[<p>As part of my efforts to <a href="/blog/2013/02/09/a-blog-is-a-mini-startup/">promote this new blog</a>, I've been submitting posts to <a href="news.ycombinator.com">hacker news</a>. Not surprisingly, my initial submissions went roughly nowhere. Less than 100 visitors saw the posts.</p>

<p>Stepping back, I decided to think a little more about timing my submissions to get better results. <a href="http://hnpickup.appspot.com/">HN Pickup Ratio</a> looked like a nice representation of the relevant factors, but it is sadly defunct, as App Engine has been blocked from scraping hacker news. I liked the concept enough to write my own implementation, which is now available here: <a href="http://hnnotify.leknarf.net">HN Notify</a>.</p>

<p>Timing my <a href="https://news.ycombinator.com/item?id=5335241">last submission</a> was exceptionally successful: it reached the front page of hacker news and was seen by 3,000 readers.</p>

<h2>Concept</h2>

<p>Assuming you want your HN submission to reach the front page, then it's important to post at times when scores on the new page are relatively high compared to the front page.</p>

<!-- more -->


<p>This assumes your goal is to reach the front page. I don't actually know how many people skim over the 'new' links, but it's fairly obvious that a far greater number of people only look at the first 30 links on the home page.</p>

<p>For the purposes of discussion, I'm going to assume a very simplistic model of the HN front page: new stories with more points will outrank older stories with less points. That is, I'm ignoring any effects existing karma has on a user's submission and any factors related to when a story gets upvotes. If we assume that a new story needs to get more points than an existing story in order to replace it on the front page, then the following is straightforward:</p>

<ul>
<li>It's a good time to submit when scores on the front page are low. If the lowest-ranked story has 10 points, it will be much easier to replace than if the lowest-ranked story has 100 points.</li>
<li>It's a good time to post when scores on the new page are high. If the highest-ranked story on the new page only has 2 points, it doesn't seem likely that your submission will fare much better.</li>
</ul>


<h2>Differences from HN Pickup</h2>

<p>HN Pickup introduced this concept of comparing the lowest-front-page with the highest-new-page scores. It graphs the mean of the last 6 data points in each category along with the ratio of the two averages. From what I've seen, it's fairly rare for new page scores to exceed those on the front page, so I'm just considering the difference between the two. I also don't think the mean is an appropriate statistic, given how easy it is for extremely popular submissions to skew the results. Instead, I'm using the second-highest and second-lowest scores. This provides some protection against outliers.</p>

<p>I disliked the idea of compulsively watching a graph update, so I added an alert mechanism. If you follow <a href="https://twitter.com/HNNotify">@HNNotify</a> on twitter, you'll be able to receive notifications of opportune submission times. It won't post more than once an hour and also ignores times when the high score on the front page is less than 10, which reduces the noise in the feed.</p>

<p>When you are compulsively waiting for a submission window, it's nice to see the chart update in real-time. This chart updates automatically, without refreshing the page.</p>

<p>Since HN Pickup was blocked, I don't want to scrape HN directly. Instead, this uses the Unofficial Hacker News API as its data source, which has been reliable so far.</p>

<h2>Architecture &amp; Implementation</h2>

<p>This sort of real-time data scraping/representation is a great fit for <a href="https://www.firebase.com/">Firebase</a>, which is a very exciting hosted database service. They provide a REST API that lets you write and read data from anywhere, which means I don't need to run a web server. Instead, the project frontend is an entirely static page running on Github Pages, which fetches updated data directly from Firebase using javascript.</p>

<p>The backend is a simple python script running on Heroku. This handles polling the HN API, writing the data to Firebase, and sending notifications to twitter.</p>

<p>The result is a very maintainable weekend project that doesn't actually require me to run any servers. Firebase, Github Pages, Heroku, and Twitter handle all of the responsibilities I'd usually need a server for, without forcing me to deal with security, scalability, or monitoring.</p>

<p>Source code is available on <a href="https://github.com/leknarf/hn-notify">Github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TechOps pre-launch checklist for web apps]]></title>
    <link href="http://leknarf.net/blog/2013/03/06/techops-pre-launch-checklist-for-web-apps/"/>
    <updated>2013-03-06T11:49:00-05:00</updated>
    <id>http://leknarf.net/blog/2013/03/06/techops-pre-launch-checklist-for-web-apps</id>
    <content type="html"><![CDATA[<p>As my startup gears up for our first paid marketing campaign, I've been giving some thought to the necessary preparations a developer should go through before driving some traffic to a web app.</p>

<p>In general, the goal isn't to prevent every possible problem. In addition to being impossible, any attempts to do so would almost certainly involve premature engineering. Instead, my goal is to be adequately prepared for unforeseeable problems. The important thing is to ensure I'll be notified when a problems occur and that I'll have enough information to investigate and fix said problems.</p>

<p>We're hosted on AWS, so this list assumes you're running in a similar environment. Managed platforms like Heroku take care of much of this for you.</p>

<p>In no particular order, the following should be completed before aggressively promoting a new web app:</p>

<!-- more -->


<h1>Prepare for disaster</h1>

<ul>
<li>Setup fully automated, zero-downtime deployments: Once people are using your site, you can't bring down the server to make changes. If you haven't already set up a smooth deployment process, this should be your first priority.</li>
<li>Automatically collect database backups and store in S3: hourly x 48, daily x 14, weekly x 8, and monthly x forever.</li>
<li>Restore the database from these backups to a developer's machine at least once.</li>
<li>Ship your log files to a hosted retention service (Splunk, Loggly, Papertrail). Or prepare your own scripts to copy files to S3.</li>
<li>Setup application performance monitoring and alerting (New Relic). If your site gets a sudden traffic spike, this should notify you that you've exceeded your capacity.</li>
<li>Server monitoring (New Relic): If your server is running out of hard drive space or if an errant process is consuming all the CPU/RAM, you'll want to get an email before the site goes down.</li>
<li>Uptime alerting/monitoring (New Relic and Pingdom): If the site does go down, you'll obviously want to know about it. New Relic does this via email, but Pingdom's mobile app is free and excellent.</li>
<li>Error alerting (Airbrake, Exceptional): Again, if something goes wrong, you want to find out about it.</li>
<li>If your application doesn't require a high level of user privacy, then you should be able to log into your site as a given user. Odd problems will pop up that only affect one user. It's a lot easier to investigate when you can see exactly what that user is seeing.</li>
</ul>


<h1>Basic security checks</h1>

<ul>
<li>Upgrade to the latest versions or install security patches for all the major components of your stack, including your framework (Rails/Django/etc.), web server (nginx, apache, etc.), and OS.</li>
<li>Install Fail2Ban on any publicly available servers.</li>
<li>Configure firewalls to restrict access to any server that don't need to be publicly available (i.e. your database, message broker, etc.) and block all but the necessary ports.</li>
<li>Change any horrendously insecure passwords you may have. Many startups use absurd passwords like "password" or "test123" for their admin accounts when starting out. These should be changed as soon as possible.</li>
</ul>


<h1>Prep for scaling</h1>

<ul>
<li>Serve static assets from S3 or a CDN (CloudFlare, CloudFront)</li>
<li>Setup a caching layer (Memcached, Redis, Varish)</li>
<li>Point your DNS entry to a load balancer, not an individual server. ELB makes this easy. If you do take on a massive amount of traffic, you can always add more web servers to the load balancer.</li>
<li>You should have the ability to scale up to N web servers at any time (Chef).</li>
<li>If something odd happens on one of your servers, the two points above make the solution easy: just spin up a new web node, add it to the load balancer, and then remove the failing node. Don't bother trying to fix or even diagnose one-off failures.</li>
</ul>

]]></content>
  </entry>
  
</feed>
